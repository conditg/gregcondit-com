Name,Slug,Collection ID,Item ID,Created On,Updated On,Published On,Thumbnail Image,Header Image,Project Description,Project Tag,Methods,Technologies
Quantum Coinflip - the most random coin flip on the internet!,quantum-coinflip,5c7abed29254e9ba7dc127ba,640753971775c6a8cafb9153,Tue Mar 07 2023 15:09:11 GMT+0000 (Coordinated Universal Time),Tue Mar 07 2023 15:11:46 GMT+0000 (Coordinated Universal Time),Tue Mar 07 2023 15:11:54 GMT+0000 (Coordinated Universal Time),https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/6407542b416166f50290140d_SCR-20230307-e4y.png,https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/6407542e42c59d0f5dfcff8b_SCR-20230307-e53.png,"<p id="""">Project Link --&gt;&nbsp;<a href=""https://www.quantumcoinflip.com/"" id="""">quantumcoinflip.com </a></p><p id="""">This is the first project I completed in quantum computing. It's a fun exploration of randomness in computing, but it actually does use real quantum hardware at IBM!<br><br>The front end was built by a colleague at AE&nbsp;Studio. </p><p>‍</p><p>‍</p><p>‍</p><p>‍</p><p>---</p>",quantum-computing,quantum-computing,python; qiskit
Predicting Dengue Fever Outbreaks,dengue-fever,5c7abed29254e9ba7dc127ba,5caa3e441aed36b8856f2d05,Sun Apr 07 2019 18:15:32 GMT+0000 (Coordinated Universal Time),Fri Apr 12 2019 21:54:39 GMT+0000 (Coordinated Universal Time),Fri Apr 12 2019 22:10:47 GMT+0000 (Coordinated Universal Time),https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5caa67331aed36445f6fbb0e_darkthumb.png,https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5caa3d6bd636b71bb15ec89b_header.png,"<h3>Overview</h3><p>Dengue Fever is a disease with severity ranging from flu-like symptoms to low blood pressure and death. Humans with Dengue Fever are not contagious; it can only be spread by Mosquitoes. It is typically observed in tropical regions,but cases have increased significantly in recent years; Health Officials and scientists are warning that climate change is likely to produce shifts that enable mosquitos to reach and infect new regions of the world. </p><p>Read on to see how I applied various Time Series techniques to predict Dengue Fever outbreaks as part of a <a href=""https://www.drivendata.org/competitions/44/dengai-predicting-disease-spread/"" data-rt-link-type=""external"">datadriven.org machine learning competition</a>. The competition data is provided and organized by the <a href=""https://www.whitehouse.gov/blog/2015/06/05/back-future-using-historical-dengue-data-predict-next-epidemic"" target=""_blank"" data-rt-link-type=""external"">Predict the Next Pandemic Initiative.</a></p><p>You can fork this project from my github repo <a href=""https://github.com/conditg/deng-ai"" data-rt-link-type=""external"">here</a>.</p><h3>Hypothesis</h3><p>Dengue Fever is spread by mosquitoes, whose breeding patterns are related to weather patterns. Therefore, the hypothesis of this competition is that an analysis of weather patterns can help predict outbreaks of the disease. </p><p>The competition provides detailed weather data for two tropical cities: San Juan, Puerto Rico (1990-2008), and Iquitos, Peru (2000-2010).</p><h3>Exploratory Data Analysis &amp;&nbsp;Pre-Processing</h3><p>If the weather drives outbreaks, one thing we'd expect to see is outbreak seasonality. </p><p>In order to visualize this, I first had to correct some date oddities. Because the week numbers given were calculated with an ISO&nbsp;standard, some years had the first week labelled as week 53, the second week labelled as week 1, the third week labelled as week 2, etc. ending in week 51. This was fixed by incrementing all week numbers for those years, then reducing any week 54's to week 1. </p><p>It does appear that there is some outbreak seasonality in these cities:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2552px"" style=""max-width:2552px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5caa78da3be19296807e4b73_seasonality.png""></div><figcaption>Note the scale:&nbsp;San Juan has a larger problem than Iquitos</figcaption></figure><p>‍</p><p>The data includes 20 weekly features that loosely fall into these categories:</p><ul><li>Temperature:&nbsp;Maximums, Minimums, Averages, and Ranges</li><li>Precipitation</li><li>Humidity</li><li>Satellite Vegetation</li></ul><p>At first glance, none of them appear to be strongly correlated with Dengue Cases:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2512px"" style=""max-width:2512px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5caa7d107177dc4c81d56484_correlations.png""></div></figure><p>‍</p><p>But Pearson correlations do not consider sequences; this only means that there are no strong relationships in the <em>current period.</em> In other words, this means a temperature change in week X&nbsp;does not correlate to reports of Dengue Fever in Week X. It says nothing about reports of Dengue Fever in Week X+1, or Week X+2.</p><p>Indeed, a seasonal look at several features (such as average temperature, below) is suggestive of sequential relationships:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2425px"" style=""max-width:2425px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5caa855d7177dce09cd571b6_temperatures.png""></div></figure><p>‍</p><p>Before starting any time series techniques, we need to have some idea how far back in time a model should consider when making predictions. Let's study the domain of Dengue Fever a bit and make our hypothesis more specific:</p><h3>What chain of events is most conducive to an outbreak of Dengue Fever?</h3><p>I broke this question into 3 parts:</p><ol><li>What weather conditions are ideal for mosquitoes to breed and hatch?</li><li>How long does it take a hatched mosquito to reach a contagious life phase?</li><li>How long after infection do Dengue Fever symptoms appear?</li></ol><h4>1. What weather conditions are ideal for mosquitoes to breed and hatch?</h4><p>The most common species of mosquito that spreads Dengue Fever, is <em>Aedes aegypti. </em></p><p>Research published in <a href=""https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2944657/#"" data-rt-link-type=""external"">Oecologia</a> [1] and in the <a href=""https://bioone.org/journals/Journal-of-the-American-Mosquito-Control-Association/volume-26/issue-1/09-5945.1/Geographic-Distribution-of-span-classgenus-speciesAedes-aegypti-span-and-span/10.2987/09-5945.1.short"" data-rt-link-type=""external"">Journal of the American Mosquito Control Association</a> [2]<em>,</em> suggests that temperature plays an important role in the viability of eggs; they have evolved best for hot and humid climates. Like all mosquitoes, their eggs need water to hatch. However, unlike other species' eggs, <em>aegypti</em> eggs can sit dormant in dry areas for a long time. Many sites (<a href=""http://www.denguevirusnet.com/life-cycle-of-aedes-aegypti.html"" data-rt-link-type=""external"">example</a>) suggest that in dry seasons, <em>aegypti </em>eggs can survive over <strong><em>a year </em></strong>without moisture, and still hatch successfully once exposed to water. Since Dengue Fever is passed from mother to offspring; this is a huge reason why Dengue Fever is so persistent.</p><p>A leading indicator for large egg hatching events might therefore be long dry periods followed by large rainfalls, or extended wet/humid seasons.</p><h4>2. How long does it take a hatched mosquito to reach a contagious life phase?</h4><p>Exposed to water in a warm climate, <em>aegypti </em>eggs develop in as little as two days. </p><p>Subsequent larvae development depends on the temperature. In their ideal climates, the larvae stage can last about a week. (Once again, during cooler periods, larvae with water access can survive for <strong><em>months.</em></strong>)</p><p>Larvae then enter the Pupae stage, which lasts about 2 days. At that point, the mosquito is an adult, and the females are able to spread disease - approximately 8-10 days from the eggs being laid.</p><h4>3. How long after infection do Dengue Fever symptoms appear?</h4><p>According to the <a href=""https://www.cdc.gov/dengue/symptoms/index.html"" data-rt-link-type=""external"">CDC </a>and the <a href=""https://www.mayoclinic.org/diseases-conditions/dengue-fever/symptoms-causes/syc-20353078"" data-rt-link-type=""external"">Mayo Clinic</a>, symptoms start 3-7 days after being bitten by an infected mosquito.</p><p>‍</p><p>My hypothesis is that given wet weather conditions following a dry period, outbreaks can occur within 2 weeks and the reports of Dengue Fever cases can be expected within 3 weeks.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5caab86a1aed365814708b02_512px-Aedes_aegypti.jpg""></div><figcaption>Image Credit:&nbsp;Muhammad Mahdi Karim [<a href=""http://www.gnu.org/licenses/old-licenses/fdl-1.2.html"" data-rt-link-type=""external"">GFDL 1.2</a>]</figcaption></figure><p>‍</p><h3>Feature Selection</h3><p>Based on the above domain knowledge and further exploratory analysis (available at my <a href=""https://github.com/conditg/deng-ai/blob/master/eda.ipynb"" data-rt-link-type=""external"">github</a>), I decided not to use some of the features, including all the vegetation index values, and selected key temperature, humidity, and rain variables for each city. For each of the techniques, I used at least 3 weeks of data to make any prediction about outbreaks.</p><p>San Juan:</p><ul><li>Maximum Temperature</li><li>Mean of (minimum temperatures, dew point temperature, and air temperature)</li><li>Relative Humidity Percent</li><li>Specific Humidity g/kg</li><li>Precipitation kg/m^2</li></ul><p>Iquitos:</p><ul><li>Mean Temperature</li><li>Minimum Air Temperature</li><li>Dew Point Temperature</li><li>Temperature Range</li><li>Specific Humidity g/kg</li><li>Precipitation amount (mm)</li></ul><p>In addition, I created boolean fields for the key seasons in each city, using the black line cutoffs noted below:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2553px"" style=""max-width:2553px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad26d56cfca99c02b7109d_boolseasons.png""></div></figure><p>‍</p><h3>Time Series Techniques</h3><p>I explored 2 types of time series solutions:</p><ol><li>Long Short Term Memory models (LSTM neural networks)</li><li>Supervised Learning models with lagged features</li></ol><h3>LSTM Training</h3><p>As with all neural networks, best practice is to train an LSTM with training data and assess it using the loss on a validation set. Most use cases allow for random holdout methods for validation set; for Time Series problems, randomizing is not valid. Validation data should be in order and occur after the training data. With large datasets, you can train on the first 66-75% of the data and validate your model with the remaining 33-25%. </p><p>In the case of this problem, which is considered a small dataset, this presents a unique challenge. Consider the below chart of the San Juan Training Data:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1439px"" style=""max-width:1439px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5caabc977177dc1ec2d608f2_validationproblem.png""></div><figcaption>My first two attempts at creating a validation holdout</figcaption></figure><p>‍</p><p>I initially tried to use the red line - or anywhere after that point - as a cut off, but my validation data would have neither of the huge outbreaks. I'd therefore consistently get validation loss that was lower than my training loss, because my validation set was never penalized for missing a large outbreak. This made it very difficult to identify overfitting.</p><p>If I instead separated the data somewhere in the blue, I was training with a very small amount of data, and was not achieving good results. </p><p>The solution is ""Walk Forward Validation"". </p><h4>Walk Forward Validation</h4><p>The best explanation of Walk Forward Validation is <a href=""https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/"" data-rt-link-type=""external"">Dr. Brownlee's (Machine Learning Mastery)</a>. In short, it entails:</p><ul><li>Training on the earliest and smallest viable training set, <strong>n</strong></li><li>Validate that model by predicting time(s) <strong>n +&nbsp;x </strong>(the observation(s) immediately following <strong>n)</strong></li><li>Update the model by training on times n+x, then repeat the two above points</li></ul><p>""Walking"" through the data like this allows you to simultaneously train the model, and validate it without messing up the order of your data. The downside is typically performance, as this requires refitting the model so many times; however, your choice of the size of <strong>x </strong>can slow or speed up the process. Below is the training process for the city with the larger history:&nbsp;San Juan. (Iquitos did not have enough training data to fit an LSTM). </p><p>My first notable training attempt was simple:&nbsp;A single LSTM layer with 16 cells, and a final fully connected linear layer to convert the layer output to my desired prediction size of one. The blue line represents the walking prediction made after training on all the data prior.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad0843832adc754d60e9e8_lstm1.png""></div></figure><p>‍</p><p>My interpretation of this chart was that the network's weights are updating too slowly. I increased the learning rate from 0.01 to 0.1:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad08b351b229136fb1b6d4_lstm2.png""></div></figure><p>‍</p><p>With the faster learning rate, our network seems to at least identify that the slope should increase around the times of the large outbreaks. I increased the LSTM layer to 64 nodes and retrained:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad091ed10c09b10c44b102_lstm3.png""></div></figure><p>‍</p><p>Even better, the predictions increase around the time of the outbreaks, but come down much too slowly. I doubled the LSTM&nbsp;layer size to 128 nodes:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad0978b32e39b8a2d84339_lstm4.png""></div></figure><p>‍</p><p>Closer, but still a long way from accurately predicting the magnitude of a bad outbreak. Unfortunately, further increases in layer size lead to overfitting and spastic predictions, such as this 256 node network that predicted negative values quite often:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad09ca832adc741d60f38a_lstm5.png""></div></figure><p>‍</p><h3>LSTM&nbsp;Performance</h3><p>Of course, there are more quantitative ways than looking at charts to validate models; the consistent prediction window size (x) used in Walk Forward Validation enables the typical loss measurements by simply evaluating the performance across all the validation windows. Interestingly, in this case, the RMSE&nbsp;(Root Mean Squared Error)&nbsp;did not change much throughout all the network architectures I tried, including any of the above that seem to be improving visually. Examining the predictions on the test data showed why.</p><p>All the model architectures I tried fell into one of three categories: </p><ol><li> Models that didn't predict any outbreaks (likely underfit)</li><li>Models that predicted nearly constant outbreaks (likely overfit)</li><li>Models that seemed to predict when an outbreak would happen, but continued predicting outbreak numbers for dozens of weeks after the fact. The error from these extended high predictions made these models unusable.</li></ol><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""880px"" style=""max-width:880px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad19636cfca9647fb6d93e_lstmtest.png""></div></figure><p>‍</p><p>Ultimately, LSTM's could not perform well for this problem. (Fortunately, the process of studying LSTM's lead me to create <a href=""/articles/lstm-ref-card"" data-rt-link-type=""page"" data-rt-link-itemid=""5c7abed29254e90ad9c127c2"" data-rt-link-collectionid=""5c7abed29254e91678c127ab"">The LSTM&nbsp;Reference Card</a>!)</p><p>‍</p><h3>Supervised Learning Models with Lagged Features</h3><p>Lagging features is a way to use certain Supervised Learning models with time series data. Each feature gets described not only by the feature observations from that time, but by the immediately prior observations as well. This can be done simply with the pandas .shift() method.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad1cad6cfca967b6b6f39e_lagged.png""></div></figure><p>‍</p><p>This method creates a lot of features, and not all of them will be useful. Therefore, it's best used with models that can penalize or ignore features, such as Lasso Regression or types of decision tree models. I used Random Forest for this project; regression methods with assumptions about normalcy or linearity did not fare well as the distribution of reported cases is an over-dispersed Poisson distribution. I lagged the selected features going back 3 weeks based on the dengue life cycle previously shown above.</p><h3>Random Forest Regressor Output</h3><p>Random Forest Regressors (using cross-validated &amp;&nbsp;grid-searched parameters) created output that passed the eye test for each city.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2369px"" style=""max-width:2369px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad1e75aac547e44d17b99b_rfrtest.png""></div></figure><p>‍</p><p>These were good for a score of about <strong>24.6 MAE</strong>, however, I had one final hypothesis that these models were under-predicting the outbreaks. With these model predictions as inputs, I did a little ""post""-processing:</p><h3>Exaggerating Outliers in Time Series Data using Basic Calculus</h3><p>The goal of this final process was to increase the ""peaks"" in the data significantly, without increasing all the predictions across the board significantly. Here's how to do this simply and automatically:</p><p>First, predict the data with a model, and take the derivative of the predictions. Then, scale the derivative values by some amount:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad21470b030aefef4cc9e0_calc1.png""></div></figure><p>‍</p><p>Next,&nbsp;find the integral of the scaled derivative (plain language:&nbsp;convert the line of slopes values back to a line of prediction values). </p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad21c48d3ca549ff3b4a0c_calc2.png""></div></figure><p>Note how while the blue Original Prediction starts with a prediction around 10, the orange Integral of the scaled derivative starts from zero; most integral functions will do this by default, since a derivative line says nothing about the scale of the initial values, just their relationship to each other (slope). Therefore, the last step is to add the starting point to ALL&nbsp;the values. In this example, we'd add 10 to every point in the orange line.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5cad2276f416764e1a7f2624_calc3.png""></div></figure><p>‍</p><p>The result is a set of predictions that increases large values but does not decrease all values!</p><p>‍</p><h3>Final Performance</h3><p>The best process I found for this data was to make predictions with Random Forest Regressors with 3 weeks of lagged features, and then to scale the final output as noted above:&nbsp;60%&nbsp;for San Juan and just 10% for Iquitos. This output scored a MAE of 24.0, which is currently in the 93rd percentile (440th/5910) on DrivenData.org.</p><p>‍</p><p><a href=""https://github.com/conditg/deng-ai"" data-rt-link-type=""external""><em>See this project on github</em></a></p><h4>Resources</h4><p>[1] Juliano SA, O'Meara GF, Morrill JR, Cutwa MM. Desiccation and thermal tolerance of eggs and the coexistence of competing mosquitoes. <em>Oecologia</em>. 2002;130(3):458–469. doi:10.1007/s004420100811</p><p>[2] ""Geographic Distribution of <em>Aedes aegypti</em> and <em>Aedes albopictus</em> Collected from Used Tires in Vietnam,"" 26(1) <br>Yukiko Higa, Nguyen Thi Yen, Hitoshi Kawada, Tran Hai Son, Nguyen Thuy Hoa, Masahiro Takagi<br>Journal of the American Mosquito Control Association (1 March 2010)</p><p>Useful LSTM&nbsp;resources:</p><p><a href=""https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/"" data-rt-link-type=""external"">Free Udacitycourse on deep learning (including RNN’s) in pytorch</a><br></p><p><a href=""https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/"" data-rt-link-type=""external"">How To Backtest Machine Learning Models for Time Series Forecasting</a></p><p><a href=""https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"" data-rt-link-type=""external"">Helpful Article on LSTM's and GRU's</a></p><p><a href=""https://twimlai.com/twiml-talk-240-the-unreasonable-effectiveness-of-the-forget-gate-with-jos-van-der-westhuizen/"" data-rt-link-type=""external"">Podcast suggesting you can trim an LSTM down to only a forget gate</a></p><p>‍</p><p>‍</p><p>‍</p>",time-series,time-series; branding; photographer; random-forest,keras-tensorflow; python
Disruptive Behavior as a Function of Screen & Outdoor Habits,disrupt,5c7abed29254e9ba7dc127ba,5ca10e88b7e3edd2981fc0bb,Sun Mar 31 2019 19:01:28 GMT+0000 (Coordinated Universal Time),Fri Apr 12 2019 21:54:56 GMT+0000 (Coordinated Universal Time),Fri Apr 12 2019 22:10:47 GMT+0000 (Coordinated Universal Time),https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca10e43f4c36e37af7f8802_children-403582_640.jpg,https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca145d0f4c36e76fb80428b_header2.png,"<h3>Overview</h3><p>This project explores the data collected by Deakin University in Melbourne. The study found relationships between screen time, outdoor time, and certain behavioral outcomes. </p><p>Read on to see how I used supervised learning to see if this data can be taken a step further to actually predict disruptive behavior in children. </p><p>This project can be forked from my github <a href=""https://github.com/conditg/predicting-disruption"" data-rt-link-type=""external"">here.</a></p><p>The study can be read in PLOS <a href=""https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0193700"" data-rt-link-type=""external"">here</a>, and the original data is available from Deakin University <a href=""https://dro.deakin.edu.au/view/DU:30106962"" data-rt-link-type=""external"">here</a>.</p><h3>Study Methods</h3><p>575 Mothers from diverse neighborhoods in Melbourne, Australia were surveyed about their child between 2-5. They were asked to report:</p><ul><li>Child's age, gender, and Mother's education level (&lt;10years, 11-13 years, or 14+&nbsp;years of school)</li><li>Child's hours per day watching TV and using the computer</li><li>Child's hours per day outside</li><li>Whether they consider the child disabled</li></ul><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2313px"" style=""max-width:2313px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca120b4b7e3ed252a1ff38f_activity-demographic.png""></div></figure><p>‍</p><p>Additionally, they reported their child's social skills using the Adaptive Social Behaviour Inventory. This involves answering 30 questions on 3 point Likert scales (Always/Sometimes/Never) related to their child's behavior. Of those questions:</p><ul><li>13 questions targeted the child's ""expressiveness"" (e.g. joins play, is open and direct, etc)</li><li>10 questions targeted the child's ""compliance"" (e.g. cooperates, is calm and easy going, etc)</li><li>7 questions targeted the child's ""disruptive"" behavior (e.g. teases, bullies, etc)</li></ul><p>Each of these 3 sub-scales was summed to create 3 scores describing the behavior of that child.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1663px"" style=""max-width:1663px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca1226fb9de9cfae4b4a34c_behavioral.png""></div></figure><p>‍</p><h3>Limitations</h3><p>Two limitations of this study should be considered:</p><ol><li><strong>Self Reporting</strong>:&nbsp;This study relies on mothers' ability to make objective assessments about their child's behavior</li><li><strong>Selection</strong>:&nbsp;The study authors made reasonable efforts to sample the population of mothers fairly, but it's still likely a bias sample; good parents may be more likely to agree to participate in a parenting study than indifferent parents, or parents with a difficult socioeconomic status. My suspicion is further heightened by the Mother's education results. In most developed countries, the percentage of adults with tertiary education is <a href=""https://en.wikipedia.org/wiki/List_of_countries_by_tertiary_education_attainment"" data-rt-link-type=""external"">20-50%</a>, not ~75% as represented in this study.</li></ol><p>‍</p><h3>Full Feature Set</h3><p>The study authors also used the Australian Government's recommendations for children's screen and outdoor time. Here is the final feature set available, including our target, ""Disrupt"". </p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""872px"" style=""max-width:872px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca12a2a34e325c1f458e082_features.png""></div></figure><p>‍</p><h3>Scenario</h3><p><strong>Let's bring this to life with a scenario: suppose a school administrator is trying to prevent classrooms from having too many disruptive students, so they are trying to identify which incoming students are likely to be disruptive and be ahead of the problem.</strong></p><p>Suppose we decide that 13 is the cutoff, so the goal is to predict which students will have a disrupt score of 13 or higher:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca12b8fb9de9c92e0b4cf2c_Target.png""></div><figcaption>33/575 students qualify as ""disruptive""</figcaption></figure><p>‍</p><h3>Baseline Model</h3><p>We'll use a Logistic Regression to classify the students as either ""Behaved"" or ""Disruptive"". Without any further work, let's examine the output of a cross-validated model and see how it does:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""894px"" style=""max-width:894px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca12d96b7e3ed9b98202bee_Baseline.png""></div></figure><p>‍</p><p>The Confusion Matrix (the left graph) tells us that while all 542 Behaved students were classified correctly, only 1 of the 33 Disruptive students were identified. </p><p>The Distributions (the center graph) show us why. scikit-learn's Logistic&nbsp;Regressor defaults to a .5 decision threshold, i.e. it will only predict ""Disrupt"" for a student if it calculates .5 or higher for that student. In this case, that's just one little red bar. For a better understanding of how to interpret these distributions in the context of the ROC curve, check out <a href=""http://www.navan.name/roc/"" data-rt-link-type=""external"">this visualization</a>.</p><p>Unlike the other two graphs, the ROC curve (the right graph) is not a description of one model, but of an entire threshold of model decision points. Right now, the model is on the far bottom left: Our False Positive Rate is zero (0/542) and our True Positive Rate is .03 (1/33). This ROC curve tells us the tradeoff we could make:&nbsp;by ""moving up the curve"", we could identify more disruptive students (i.e. increase True Positives) but we would also falsely accuse behaved students more often (i.e. increase False Positives). </p><p>This is a bad model. It's F1 Score, a score which factors both precision and recall, is only .06.</p><p>This is a classic <strong>Class Imbalance </strong>problem! our Logistic regressor converges on a solution that favors Behaved students, just because there are so many more of them. We need the model to view the two groups more equally.</p><h3>Fixing Class Imbalance with weighting</h3><p>One option is to weight the data points in proportion to their representation in the dataset. In this case, the error on a Disruptive student counts 16x more (542/33) than an error on a Behaved student. Let's see how that does:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""894px"" style=""max-width:894px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca1313ef4c36ea66a7ffa37_balanced.png""></div></figure><p>‍</p><p>We've still only identified 5/33 Disruptive students, and what's worse, we've now falsely accused 42 Behaved students of being disruptive. The Distribution of predictions are all very close to .5, which indicates that this model is generally unsure of everything. Balanced class weighting may have been an overreaction.</p><p>Despite poor results, the F1 Score increased to 0.12 in this model.</p><p>Rather than guessing, let's search for the right class weight to use by plotting F1 Scores for all class weight options:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca13228b7e3ed85ff203c1f_F1search.png""></div></figure><p>‍</p><p>Although F1-score peaks at 7, that appears to be a volatile region. 19 is a much more stable peak, and is also closer to the true class ratio in our data, which is 16:1. Let's try class weights of 19:1:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1234px"" style=""max-width:1234px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca132d3f4c36e72f37ffef0_19to1.png""></div></figure><p>‍</p><p>Our F1 Score has increased to .20. For the first time, the model is erring on the side of predicting ""Disrupt"" rather than not. This is how the distributions <em>should </em>look:&nbsp;some samples near 0 (definitely Behaved), some near 1 (Definitely Disruptive), and some near the middle. The goal of a good model is to reduce the overlap between red and green as much as possible.</p><p>‍<br>At this point, we could tell the school something like this, based on the ROC curve above and choosing the decision threshold circled in red:</p><blockquote>""Our current model could correctly identify ~85% of disruptive students; however, it would also result in almost 40% of well-behaved students being mistakenly labelled as disruptive"".</blockquote><p>Obviously, this is not a usable solution - there are way too many false positives. </p><h3>Removing <strong>Collinearity</strong></h3><p>Continuous variables, like TV time or Outdoor Time, are correlated with each other. Having correlated features violates an assumption of many generalized linear models, including Logistic Regression. In this case Principal Components Analysis can be used to remove <strong>multicollinearity</strong> without losing any information, but at the expense of interpretability. </p><p>Principal Components Analysis deserves it's own post (and will get one, coming soon!), but the short version is that PCA&nbsp;redefines the basis system we’re using to measure our data. The new basis, or ""coordinates"", for each point will still describe all the variability in the original dataset, but the dimensions will be orthogonal to each other so there's no multicollinearity. </p><p>PCA&nbsp;works best for moderately correlated features. In this case, the following groups were respectively transformed to components:</p><ul><li>Age, Comply, Express</li><li>tvTime, cpuTime, outdoorTime</li></ul><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""894px"" style=""max-width:894px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca13912b7e3eda02f205f3a_19t01PCA.png""></div></figure><p>‍</p><p>With these new features, we were able to identify one additional Disruptive student successfully. This may not seem like much, but given that we did that with no new information, it's pretty neat!</p><p>‍</p><h3>More Boolean Features and PCA&nbsp;Components</h3><p>The last feature engineering step that improved outcomes was to create some additional binary variables. </p><ol><li>Whether or not the child's screen time measurements were BOTH (TV and Computer) over the average</li><li>Whether their expressiveness score was above average</li><li>Whether their compliance score was above average</li><li>Mother's education (either 14+years or not)</li></ol><p>These were combined with the other binary features in the dataset:</p><ol><li>Disability</li><li>Screen Time Requirement Met</li><li>Physical Requirement Met</li><li>Gender</li></ol><p>All 8 of these were then transformed into PCA components.</p><p>PCA&nbsp;on binary variables is a different beast; I'll have an article about this process up soon. One cool difference is that the binary nature of the original features enables easy plotting of the first 2 PCA&nbsp;components, and interpretation of what compose them. Below is the same plot 8 times:&nbsp;the first 2 PCA&nbsp;components. Each plot has a different color scheme highlighting one of the original 8 features listed above. You can actually see which features compose the first two components!</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""874px"" style=""max-width:874px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca13ba8b7e3edc5492062fa_scatters.png""></div><figcaption>First 2 PCA&nbsp;components created from binary features</figcaption></figure><p>‍</p><p>While PCA components are normally not very interpretable, in this case we can actually see that disability explains a lot of the variance in this data, while Mother's Education does not (at least not in the first 2 components).</p><p>Anyway - after adding the Binary features, I plotted F1 scores again, and this time the best score used weights of 7:1, and also appeared pretty stable:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca13cb4b7e3ed475320644b_F1searchbinary.png""></div></figure><p>‍</p><h3>Final Model</h3><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""894px"" style=""max-width:894px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca13cd14ccfeb43aac109ab_finalmodel.png""></div></figure><p>‍</p><p>This model has an F1 score of .24, which is the best I could do with this data. Here's a summary of the models tried, including a couple I didn't highlight here:</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""980px"" style=""max-width:980px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5ca13d27b7e3ed57fd206505_summary.png""></div></figure><p>‍</p><h3>Summary</h3><p>Screen time and outdoor time clearly have correlations with behavior, as was identified in the original study. Additionally, predictive methods <em>could </em>be used to identify extreme outliers. In general, however, more data would need to be collected to make confident predictions about a disruptive behavior.</p><p><a href=""https://github.com/conditg/predicting-disruption"" data-rt-link-type=""external""><em>See this project on github</em></a></p><p>‍</p>",photographer,logistic-regression; photographer,scikit-learn; python
Facial Recognition with a Restricted Boltzmann Machine,rbm,5c7abed29254e9ba7dc127ba,5c7abed29254e9018ac127c3,Sun Dec 13 2015 22:25:15 GMT+0000 (Coordinated Universal Time),Thu Apr 11 2019 01:30:34 GMT+0000 (Coordinated Universal Time),Thu Apr 11 2019 01:33:24 GMT+0000 (Coordinated Universal Time),https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9ea357b9de9c3443af34e3_rbmmain.png,https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9ea35ab9de9c4f8faf34e4_rbmImage.png,"<h3>Overview</h3><p>Have you ever wondered how facial recognition works?</p><p>With text or number-based tasks in machine learning, we can teach our machine what is important; for example, if I wanted to predict your income, I could teach the machine to learn from important ""features"", like your education, age, and career. What about images? What ""features"" can a machine learn to recognize your face?</p><p>This analysis uses the <a href=""http://vision.ucsd.edu/content/yale-face-database"" target=""_blank"" data-rt-link-type=""external"">Yale Face Database</a> to generate and examine features that can predict whose face is in a given picture. The goal is to build an intuition for RBM&nbsp;output on image datasets.</p><p>You can fork this project from my github repo <a href=""https://github.com/conditg/rbm-feature-extraction"" target=""_blank"" data-rt-link-type=""external"">here</a>.</p><h3>Image Description</h3><p>The <a href=""http://vision.ucsd.edu/content/yale-face-database"" target=""_blank"" data-rt-link-type=""external"">Yale Face Database</a> contains 165 grayscale images in GIF format of 15 individuals. There are 11 images per subject, one per different facial expression or configuration: center-light, w/glasses, happy, left-light, w/no glasses, normal, right-light, sad, sleepy, surprised, and wink.</p><p>Credit also goes to the creators of <a href=""http://vismod.media.mit.edu/vismod/classes/mas622-00/datasets/"" target=""_blank"" data-rt-link-type=""external"">this normalized version</a> of this dataset. The Centered versions of the images are what are used in the below analysis. Here's a few selected examples:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1411px"" style=""max-width:1411px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9ea9814ccfeb7953ba781f_examples.png""></div><figcaption><em>Happy, left-light, and w/glasses examples</em></figcaption></figure><p>‍</p><h3>How Do Computers ""See"" Images?</h3><p>Simple - pixels! More specifically, computers see a huge matrix of numbers that describe the color content of each pixel in an image. These images are gray-scale, so each pixel is assigned a number that describes how bright the pixel is:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1724px"" style=""max-width:1724px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9ed7c9b9de9c9c78afbccf_cpuvisionlong.png""></div></figure><p>‍</p><p>If the image is colored, it's only a little more complicated:&nbsp;each pixel gets 4 numbers representing the <a href=""https://en.wikipedia.org/wiki/RGBA_color_space"" target=""_blank"" data-rt-link-type=""external"">RGBA</a> color channels. These 4 numbers would describe how much Red, Blue, Green, and Alpha (transparency) compose that pixel. Most operations done on images are therefore just linear algebra computations.</p><p>Again, the goal is to have some features we can teach a machine to look for. Since a machine just sees a matrix, we need some way for it to learn in broad strokes - we don't want to do anything pixel-by-pixel. This is where Deep Learning comes in.</p><p>‍</p><h3>Restricted Boltzmann Machine</h3><figure class=""w-richtext-figure-type-image w-richtext-align-floatright"" data-rt-type=""image"" data-rt-align=""floatright"" data-rt-max-width=""50%"" style=""max-width:50%""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9eec27574124c941141485_rbmdiagram.png""></div><figcaption>RBM&nbsp;Architecture</figcaption></figure><p>A Restricted Boltzmann Machine (RBM)&nbsp;is a Neural Network with only 2 layers:&nbsp;One visible, and one hidden. </p><p>The visible layer is the inputs; in this case, the images. The hidden layer will ultimately become information about useful features if training is successful.</p><p>For a deeper dive into how RBM's work, I like this <a href=""https://www.youtube.com/watch?v=yo3RSeWlgns"" target=""_blank"" data-rt-link-type=""external"">video</a>; for now, here's a simpler way to think about it. The input layer sends our image data to the hidden layer, and the hidden layer tries to describe it back to the input layer. The catch is that the hidden layer is ""restricted""; the nodes in that layer cannot communicate with each other; each can only communicate with the input layer's nodes. </p><p>Like other neural nets, training involves some number of these epochs. Each epoch, the input layer uses some loss function to describe how poorly the hidden layer is doing and suggest improvements to each node.</p><p>The hope is that after enough training passes, each of the nodes in the hidden layer will have ""focused"" on certain image characteristics that contain information about what's in the image. </p><p>You <em>could </em>think about it this way: perhaps one node could learn to focus on eye color, one on nose shape, etc... That wouldn't be too far off conceptually, except that the features found by nodes are unlikely to be so intuitively human, as we'll see soon.</p><p>Once trained, the hidden layer will hold features that can be used to make predictions about brand new images. Let's see a Python implementation.</p><p>‍</p><h3>RBM in scikit-learn</h3><p>We can use skimage.io to read the images into memory:</p><p>‍</p><p>&lt;p&gt;CODE:&nbsp;https://gist.github.com/conditg/6f4937e06226ed681c7b8e94f6baae57.js&lt;/p&gt;</p><p>‍</p><p>""imgs"" is now a list of arrays that each represent an image.</p><p>Before feeding these into an RBM, consider resizing them. Right now they are 231x65, or over 38 thousand pixels each. If we shrink to 77x65, we reduce that to about 5 thousand pixels, which can significantly improve the speed of training. Finally, we want to flatten our input from a 77x65 to a single 2D list of 1x5005, so the RBM receives a flat input layer:</p><p>‍</p><p>&lt;p&gt;CODE: https://gist.github.com/conditg/baa236b6e833aae60e282aaf464b204c.js&lt;/p&gt;</p><p>‍</p><p>Next, we can set up a Pipeline that feeds the data through an RBM, and then the output can be used by a Logistic Regression to classify the different faces.<em> </em></p><p><em>‍</em>We'll use a learning rate of .01. Just like any other neural net, this refers to how quickly the nodes react to ""suggestions"" from the loss function. In this case we'll create 150 nodes in the hidden layer, which means we'll get 150 features to use to predict who's in each picture.</p><p>‍</p><p>&lt;p&gt;CODE: https://gist.github.com/conditg/f1a7ebb45a2ad3d2f8ab11315c22f97a.js&lt;/p&gt;</p><p>‍</p><p>Before we actually feed this pipeline to our images, we also need to feed it labels, as Logistic Regression is a supervised learning technique. The Yale images are named in a certain order, so you can create a target variable this way:</p><p>‍</p><p>&lt;p&gt;CODE:&nbsp;https://gist.github.com/conditg/e3d29003a17a4cecafac1e1ca5e51b49.js&lt;/p&gt;</p><p>‍</p><p>We're ready to train the pipeline:</p><p>‍</p><p>&lt;p&gt;CODE:&nbsp;https://gist.github.com/conditg/bcf3ca0c300cf8f13cdddd2a440ac3d2.js&lt;/p&gt;</p><p>‍</p><p>Now we have a model. Before we take a closer look at what the RBM has done, we can predict the training data. (Overfitting applies here, so as always, testing a model on the same data it trained on is poor practice. This project is really about understanding RBM's and facial recognition, so we'll take the shortcut.)</p><p>‍</p><p>&lt;p&gt;CODE: https://gist.github.com/conditg/64d791fce4d8ef3d67c6e21c5d0d1702.js&lt;/p&gt;</p><p>‍</p><p>You can see the full output of this report <a href=""https://github.com/conditg/rbm-feature-extraction/blob/master/YaleFaces.ipynb"" data-rt-link-type=""external"">here</a>, but this model correctly identifies the person in 96% of the training images.</p><h3>Examining the Features Created</h3><p>With the model fitted, we can access the hidden layer by calling the components attribute. As a reminder, these will be computer-representations of images: big matrices. The below code accesses all 150 components and converts them to images we can examine.</p><p>‍</p><p>&lt;p&gt;CODE:&nbsp;https://gist.github.com/conditg/473c6010f029d46029d8ba8d5a16f7bc.js&lt;/p&gt;</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""809px"" style=""max-width:809px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9ee4bfb9de9ca811afe6cb_150.png""></div><figcaption>150 features identified by an RBM for facial recognition</figcaption></figure><p>‍</p><p>We can take a closer look at a few of these:</p><p>‍</p><p>&lt;p&gt;CODE:&nbsp;https://gist.github.com/conditg/e8fb534d22d3d5bae2cdc1ab06667c35.js&lt;/p&gt;</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""810px"" style=""max-width:810px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9ee590b9de9c5a82afe817_3.png""></div><figcaption>Selected features identified by an RBM for facial recognition</figcaption></figure><p>‍</p><p>For some hidden components, like the left and center examples above, it's intuitive to see how they are storing information about image content, and how they contribute to recreations of the original (during training) or predictions. Others, like the right, are less intuitive. This is NOT&nbsp;an indication of usefulness; even if a hidden component is not intuitively explaining patterns to the human eye, it can still contain information that a machine can process, and can combine with other hidden components to make accurate predictions.</p><p><a href=""https://github.com/conditg/rbm-feature-extraction"" data-rt-link-type=""external""><em>See this project on github</em></a></p>",branding,branding; web-design; logistic-regression; facial-recognition,scikit-learn; skimage
Natural Language Processing of Grantland Articles,nlp-grantland,5c7abed29254e9ba7dc127ba,5c7abed29254e9b952c127c1,Thu Jan 07 2016 20:01:21 GMT+0000 (Coordinated Universal Time),Sun Mar 31 2019 22:24:28 GMT+0000 (Coordinated Universal Time),Sun Mar 31 2019 22:24:57 GMT+0000 (Coordinated Universal Time),https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c97f8587e3bc986607d8eee_2019-03-24%2016_35_30-%C2%BB%20Sports%20and%20Pop%20Culture%20from%20our%20rotating%20cast%20of%20writers%20%E2%80%93%20Grantland.png,https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9801e769291200bbb9489c_GrantLandLong.png,"<h3>Overview</h3><p>Grantland was a long form blog owned by ESPN. Grantland was known for its award-winning writing, and it’s contributors brilliantly mixed sports, popular culture, and data analytics &amp; visualization into riveting stories and analysis. Grantland was writing and media as it should be, and it was shut down because it was much harder to monetize than low-effort clickbait. This analysis is a lighthearted tribute to the site and it’s contributors. </p><p>Read on to see how I used unsupervised clustering methods to group articles by topic, and supervised learning methods to predict the author of each article.</p><p>This project can be forked from my github <a href=""https://github.com/conditg/nlp-grantland"" data-rt-link-type=""external"">here</a>.</p><h3>Corpus and Collection Process</h3><p>I scraped 126 Articles by selected contributors from Grantland.com using BeautifulSoup.</p><p>Contributors were chosen fairly arbitrarily by my memory of which writers I enjoyed most,which means the text will bias towards my interests, that is, mostly basketball and occasionally football. </p><p>I also added a few extra contributors at random for variety. </p><p>The scraper pulled the most recent 10 posts for each contributor, then excluded podcast posts. Bill Simmons’s most recent posts were almost all podcasts, so his were selected manually. Ultimately, the corpus broke down by contributor this way:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c980296f5786d2e54a16410_glcorpus.png""></div></figure><p>‍</p><h3>Pre-processing</h3><p>Before any features could be created, I had to remove special characters and expand contractions into full words. I also removed ""stop words"" - simple words like ""and"" or ""the"" that are common and don't contain meaningful information (on their own)&nbsp;in the context of a sports/pop culture blog.</p><p>I then chose to use lemmas instead of the original text. Lemmas can be thought of as the ""reduced"" form of a word; for example, ""decide"", ""decides"", and ""decided"" would all be reduced to the base ""decide"". Remember - to a machine, words are just groups of characters, and a machine can't tell that ""decide"" and ""decides"" indicate similar ideas any more than it can tell that ""right"" and ""rights"" have different meanings. That's why lemmas can improve NLP&nbsp;models in many cases - we lose a small degree of context, but indicate to the machine that words from the original text convey near-identical ideas.</p><p>NLP&nbsp;packages have these word-to-lemma relationships mapped out thoroughly for general -audience text; I used <a href=""https://spacy.io/"" data-rt-link-type=""external"">spacy </a>in this case.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""971px"" style=""max-width:971px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c97fb6ede94e84bbc0454bc_w2v.png""></div><figcaption>Pre-processing</figcaption></figure><p>‍</p><h3>Vectorization</h3><p>I then trained <a href=""https://radimrehurek.com/gensim/"" data-rt-link-type=""external"">gensim</a>'s doc2vec model on 94 of the 126 articles, and held out the rest as a test set. The model was set to output a vector of length 65, and was trained for 100 epochs. I&nbsp;ignored any words that appeared less than 7 times. The result was a 65-dimension vector describing each article based on it's contents; let's break this down. </p><p>In a <strong>word</strong>2vec model, each word is assigned a vector of 65 numbers, which are initialized completely randomly neural network. In each epoch, each word's vector is iteratively nudged such that the 65-dimensional ""coordinates"" (or weights) for a word's vector are nearby other words that are used in similar contexts, and are further from words used in very different contexts. If it is well trained, we would expect the vector for ""basketball"" to be close/similar to the vector for ""game"", and far from the vector for ""movie"" or ""goalie"". </p><p>The word2vec model is useful for making predictions at the word-level, but the goal here is to make predictions about entire articles, so we need a doc2vec model. The doc2vec algorithm starts as a word2vec; at the end, it creates an additional vector for the article itself, and weights it appropriately based on the vectors of all the words within it. </p><p>‍</p><h3>3 Informative Clustering Solutions to predict the topic of each article</h3><p>With 65 features to describe each article, Our hope is that we have created features that can make meaningful differentiations. However, we don't know how many topics there are to differentiate. Below are 3 different clustering methods I tried, which were each insightful and revealed different things about the data.</p><h4>K-Means Clustering</h4><p>K-Means is a great place to start due to it's simplicity; however, it has 2 key limitations that matter:</p><ul><li>K-Means creates clusters based on centroids, so it assumes the clusters are all the same size. I suspect this is not valid, as it's unlikely I happened to select the same number of articles from each topic.</li><li>K-Means requires that you know how many clusters you are searching for; I did not.</li></ul><p>One way to estimate how many clusters are present in the data is to calculate silhouette scores for different numbers of K-means clusters. Silhouette score is a crude calculation for how different the clusters are from each other:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""786px"" style=""max-width:786px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c980b476929122e3db964b6_glsilhouette.png""></div><figcaption>The Silhouette score peaks at 4 and slowly declines as we use more and more clusters.</figcaption></figure><p>‍</p><p>Based on the above, I started with 4 clusters, but quickly moved to 3 as it appeared to be a cleaner option. Obviously, we can't visualize 65-dimensional spaces, so I used Principal Components Analysis to capture the 2 dimensions that explained the greatest amount of variance, and we see that this method has done really clean job separating the 3 clusters, even in just the first 2 dimensions!</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9e2dfdc7170a49bbfac9be_Kmeans2D.png""></div></figure><p>‍</p><p>Let's look at word clouds to see what each of these clusters contain:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2237px"" style=""max-width:2237px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9e2fb0670f524da5b6dbda_KmeansClusters.png""></div><figcaption>Most common words in the 3 clusters identified by K-Means</figcaption></figure><p>‍</p><p>Not bad!&nbsp;Loosely, The first topic is basketball, the second is football, and the 3 seems to be a generic ""pop culture"" cluster. This is a great start. </p><h4>Spectral Clustering</h4><p>However, the assumption that each cluster is the same size doesn't sound right; I read about basketball more often than I do football, and MUCH more often than pop culture topics. It's likely that my choice of authors would have skewed towards basketball articles. </p><p>Spectral Clustering still requires that we specify how many clusters we want to see, but because it uses eigenvectors of the Laplacian matrix of a similarity graph, Spectral clustering can identify clusters of different sizes - this matches our intuition better. </p><p>Once again, I used 3 clusters, and used PCA&nbsp;to visualize the best 2 dimensions:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9e31b6670f52b474b6e57c_Spectral2D.png""></div></figure><p>‍</p><p>As expected, we have various sizes of clusters, which indicates that we have topics in varying quantities. In 2D, we have more overlap then before, which is not desirable. Let's examine the word clouds of each:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2234px"" style=""max-width:2234px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9e32a8c7170a202efaea6e_SpectralClusters.png""></div><figcaption>Most common words in the 3 clusters identified by Spectral Clustering</figcaption></figure><p>‍</p><p>This is VERY&nbsp;insightful. Let's break it down. The first cluster is still primarily basketball, but is a bit less focused now; we see words like ""people, thing, feel, go"" which previously were present in the pop culture topic. The second cluster is still football, and is perhaps more specific to football. Finally, the third topic is completely new - Hockey!</p><p>The interpretation here is that 3 clusters is the wrong number. One solution is missing hockey, and the other is missing pop culture. Those articles are being with with one of the other topics. </p><h4>Affinity Propagation</h4><p>Affinity Propagation can identify a variable number of clusters AND&nbsp;clusters of varying sizes. The downside is that this solution is very sensitive to differences, and tends to really overestimate how many clusters exist. When we run Affinity Propagation on this data, it returns 11 clusters! In 2 dimensions, they are not well separated, either:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9e3511670f523ffbb6fbbc_Affinity2D.png""></div></figure><p>‍</p><p>However - it's an interesting starting point. From there I&nbsp;examined word clouds of each of the 11, and realized I could consolidate down to a 6-cluster solution that is really clean! With more data, I suspect the especially bizarre ""Future"" cluster would not remain consistent - it strikes me as a catch-all cluster for some irreverent outliers, whereas the other 5 are well defined and consistent:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1637px"" style=""max-width:1637px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9e35e16f46c444b8b8a7b3_AffinityClusters.png""></div><figcaption>Most common words in the 6 clusters consolidated from the 11 clusters identified by Affinity Propagation</figcaption></figure><p>‍</p><h3>Predicting the Author of each Article</h3><p>Unlike topics, we know the authors in advance, so predicting authorship is a supervised problem. To predict Author, I tried 3 Classification techniques:</p><ul><li>Logistic Regression</li><li>K-Nearest Neighbors</li><li>Random Forest</li></ul><p>Additionally, I tried two different sets of features to describe the data: </p><ul><li>doc2vec (the same vectors used in clustering above)</li><li>Latent Semantic Analysis</li></ul><p>Latent Semantic Analysis (LSA) is a technique that uses Singular Value Decomposition on a sparse matrix containing term counts per document. Ultimately, the output is a feature vector for each document with information about the concepts within.</p><p>I used F1 scores to compare the methods, which is a handy way of ensuring that models are performing well on both precision and recall.</p><p>Despite the success of clustering on doc2vec vectors, those features performed poorly for classification: For all 3 techniques, LSA features were more successful. </p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9e3d0c032049d2ab0fe8f3_NLPModels.png""></div></figure><p>‍</p><p>Additionally, Logistic Regression outperformed the other 2 techniques regardless of the feature set. The performance of the best solution is below:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""504px"" style=""max-width:504px""><div><img src=""https://uploads-ssl.webflow.com/5c7abed29254e9f90fc127aa/5c9e3d5184c7f7af012fe793_LSA_Logistic_CM.png""></div></figure><h3>Just For Fun</h3><p>I extracted the most used words and most unique words used by each contributor - this can be viewed at the bottom of <a href=""https://github.com/conditg/nlp-grantland/blob/master/NLP-Grantland.pdf"" data-rt-link-type=""external"">this presentation</a>. Enjoy!</p><p><a href=""https://github.com/conditg/nlp-grantland"" data-rt-link-type=""external""><em>See this project on github</em></a></p>",natural-language-processing,web-design; natural-language-processing; classification; logistic-regression; clustering,beautifulsoup; python; gensim; scikit-learn
